{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b8e6638"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import kagglehub\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "# # --- Device Configuration ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# # # --- Data Preparation \n",
        "# print(\"Downloading dataset...\")\n",
        "# path = kagglehub.dataset_download(\"olaolabenjo/englishyoruba-parallel-corpus-sample\")\n",
        "# print(f\"Dataset downloaded to: {path}\")\n",
        "\n",
        "df = pd.read_csv(\"yoruba_english_parallel_sample.csv\")\n",
        "english_sentences = df[\"english\"].astype(str)\n",
        "yoruba_sentences = df[\"yoruba\"].astype(str)\n",
        "\n",
        "# Create sample_data directory if it doesn't exist\n",
        "if not os.path.exists(\"/content/sample_data\"):\n",
        "    os.makedirs(\"/content/sample_data\")\n",
        "\n",
        "# Save english_sentences and yoruba_sentences to temporary files for CBOW training\n",
        "english_sentences.to_csv(\"/content/sample_data/english.txt\", index=False, header=False)\n",
        "yoruba_sentences.to_csv(\"/content/sample_data/yoruba.txt\", index=False, header=False)\n",
        "print(\"English and Yoruba sentences saved to /content/sample_data/.\")\n",
        "\n",
        "\n",
        "# --- Vocabulary Building Functions ---\n",
        "def build_vocab(sentences: list, min_freq: int = 1):\n",
        "    tokens = []\n",
        "    for s in sentences:\n",
        "        tokens.extend(s.lower().split())\n",
        "    freq = Counter(tokens)\n",
        "    vocab = {word: idx for idx, (word, c) in enumerate(freq.items()) if c >= min_freq}\n",
        "    return vocab\n",
        "\n",
        "def build_vocab_seq2seq(sentences, min_freq=1, specials=[\"<pad>\", \"<sos>\", \"<eos>\"]):\n",
        "    counter = Counter()\n",
        "    for s in sentences:\n",
        "        counter.update(s.split())\n",
        "\n",
        "    vocab = {tok: idx for idx, tok in enumerate(specials)}\n",
        "    idx = len(specials)\n",
        "\n",
        "    for word, freq in counter.items():\n",
        "        if freq >= min_freq and word not in vocab:\n",
        "            vocab[word] = idx\n",
        "            idx += 1\n",
        "\n",
        "    return vocab\n",
        "\n",
        "# --- CBOW Model and Training (Adapted for GPU) ---\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.emb(x)          # [B, context, D]\n",
        "        mean = embeds.mean(dim=1)     # CBOW\n",
        "        return self.linear(mean)\n",
        "\n",
        "class CBOWDataset(Dataset):\n",
        "    def __init__(self, sentences: list, vocab: dict, window: int =2):\n",
        "        self.data = []\n",
        "        self.vocab = vocab\n",
        "\n",
        "        for s in sentences:\n",
        "            words = s.lower().split()\n",
        "            for i in range(window, len(words) - window):\n",
        "                context = words[i-window:i] + words[i+1:i+window+1]\n",
        "                target = words[i]\n",
        "\n",
        "                if target in vocab and all(w in vocab for w in context):\n",
        "                    self.data.append(\n",
        "                        ([vocab[w] for w in context], vocab[target])\n",
        "                    )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context, target = self.data[idx]\n",
        "        return torch.tensor(context), torch.tensor(target)\n",
        "\n",
        "def train_cbow(sentences, save_path: str, embedding_dim: int = 50, epochs: int = 1000):\n",
        "    vocab = build_vocab(sentences)\n",
        "    dataset = CBOWDataset(sentences, vocab)\n",
        "    loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "    model = CBOW(len(vocab), embed_dim=embedding_dim).to(device) # Move model to device\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    print(f\"Starting CBOW training for {save_path}...\")\n",
        "    for epoch in range(epochs):\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device) # Move tensors to device\n",
        "            loss = loss_fn(model(x), y)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f\"Epoch {epoch} Loss {loss.item():.4f}\") # Comment out to reduce output\n",
        "\n",
        "    torch.save({\n",
        "        \"embeddings\": model.emb.weight.data.cpu(), # Save embeddings back to CPU\n",
        "        \"vocab\": vocab\n",
        "    }, save_path)\n",
        "    print(f\"CBOW embeddings saved to {save_path}\")\n",
        "\n",
        "\n",
        "# --- Seq2Seq Model Classes (num_layers=4 and GPU adaptation for Encoder output) ---\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers=4): # num_layers=4\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Add two parallel bidirectional LSTMs\n",
        "        self.lstm1 = nn.LSTM(emb_dim, hidden_dim, num_layers=self.num_layers, batch_first=True, bidirectional=True)\n",
        "        self.lstm2 = nn.LSTM(emb_dim, hidden_dim, num_layers=self.num_layers, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "\n",
        "        # Process through both LSTMs\n",
        "        _, (h1, c1) = self.lstm1(emb)\n",
        "        _, (h2, c2) = self.lstm2(emb)\n",
        "\n",
        "        # Initialize combined hidden and cell states on the same device as input 'x'\n",
        "        batch_size = x.size(0)\n",
        "        combined_h = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=x.device)\n",
        "        combined_c = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=x.device)\n",
        "\n",
        "        # Combine hidden and cell states\n",
        "        for i in range(self.num_layers):\n",
        "            # Sum forward and backward components for lstm1 for current layer\n",
        "            h1_layer_combined = h1[2 * i, :, :] + h1[2 * i + 1, :, :]\n",
        "            c1_layer_combined = c1[2 * i, :, :] + c1[2 * i + 1, :, :]\n",
        "\n",
        "            # Sum forward and backward components for lstm2 for current layer\n",
        "            h2_layer_combined = h2[2 * i, :, :] + h2[2 * i + 1, :, :]\n",
        "            c2_layer_combined = c2[2 * i, :, :] + c2[2 * i + 1, :, :]\n",
        "\n",
        "            # Sum the combined states from both LSTMs\n",
        "            combined_h[i, :, :] = h1_layer_combined + h2_layer_combined\n",
        "            combined_c[i, :, :] = c1_layer_combined + c2_layer_combined\n",
        "\n",
        "        return combined_h, combined_c\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers=4): # num_layers=4\n",
        "      super().__init__()\n",
        "      self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "      self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=num_layers, batch_first=True) # num_layers=4\n",
        "      self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "  def forward(self, x, h, c):\n",
        "      emb = self.embedding(x)\n",
        "      outputs, (h, c) = self.lstm(emb, (h, c))\n",
        "      return self.fc(outputs), h, c\n",
        "\n",
        "\n",
        "# --- Seq2Seq Dataset and Dataloader ---\n",
        "class seq2seqDataset(Dataset):\n",
        "    def __init__(self, eng, yor, eng_vocab, yor_vocab):\n",
        "        self.pairs = []\n",
        "        for e, y in zip(eng, yor):\n",
        "            e_tokens = e.lower().split()\n",
        "            y_tokens = y.lower().split()\n",
        "\n",
        "            # Filter for words in vocab and add <sos>, <eos>\n",
        "            e_ids = [eng_vocab[\"<sos>\"]] + \\\n",
        "                    [eng_vocab[w] for w in e_tokens if w in eng_vocab] + \\\n",
        "                    [eng_vocab[\"<eos>\"]]\n",
        "\n",
        "            y_ids = [yor_vocab[\"<sos>\"]] + \\\n",
        "                    [yor_vocab[w] for w in y_tokens if w in yor_vocab] + \\\n",
        "                    [yor_vocab[\"<eos>\"]]\n",
        "\n",
        "            self.pairs.append((e_ids, y_ids))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.pairs[idx][0], dtype=torch.long), \\\n",
        "               torch.tensor(self.pairs[idx][1], dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "def collate_batch(batch):\n",
        "    src_list, tgt_list = [], []\n",
        "    for _src, _tgt in batch:\n",
        "        src_list.append(_src)\n",
        "        tgt_list.append(_tgt)\n",
        "\n",
        "    # Pad sequences to the length of the longest sequence in the batch\n",
        "    src_padded = torch.nn.utils.rnn.pad_sequence(src_list, batch_first=True, padding_value=eng_vocab_seq2seq[\"<pad>\"])\n",
        "    tgt_padded = torch.nn.utils.rnn.pad_sequence(tgt_list, batch_first=True, padding_value=yor_vocab_seq2seq[\"<pad>\"])\n",
        "    return src_padded, tgt_padded\n",
        "\n",
        "\n",
        "# --- Seq2Seq Training Function (Adapted for GPU) ---\n",
        "def train_seq2seq(encoder, decoder, loader, epochs: int = 1000):\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=yor_vocab_seq2seq[\"<pad>\"]) # Ignore padding in loss calculation\n",
        "    optim = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()))\n",
        "\n",
        "    print(\"Starting seq2seq training...\")\n",
        "    for epoch in range(epochs):\n",
        "        for src, tgt in loader:\n",
        "            src, tgt = src.to(device), tgt.to(device) # Move tensors to device\n",
        "\n",
        "            h, c = encoder(src)\n",
        "            output, _, _ = decoder(tgt[:, :-1], h, c)\n",
        "\n",
        "            loss = loss_fn(\n",
        "                output.reshape(-1, output.shape[-1]),\n",
        "                tgt[:, 1:].reshape(-1)\n",
        "            )\n",
        "\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "        print(f\"Epoch {epoch} Loss {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "# --- Translation Function (Adapted for GPU) ---\n",
        "def translate(sentence, encoder, decoder, eng_vocab, yor_vocab, max_len=20):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    inv_vocab = {v:k for k,v in yor_vocab.items()}\n",
        "\n",
        "    src = torch.tensor([[eng_vocab[w] for w in sentence.lower().split() if w in eng_vocab]], device=device) # Move input to device\n",
        "    h, c = encoder(src)\n",
        "\n",
        "    token = torch.tensor([[yor_vocab[\"<sos>\"]]], device=device) # Move input to device\n",
        "    result = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        out, h, c = decoder(token, h, c)\n",
        "        idx = out.argmax(-1).item()\n",
        "        if inv_vocab[idx] == \"<eos>\":\n",
        "            break\n",
        "        if inv_vocab[idx] == \"<pad>\":\n",
        "            token = torch.tensor([[idx]], device=device) # Ensure new token is on device\n",
        "            continue\n",
        "        result.append(inv_vocab[idx])\n",
        "        token = torch.tensor([[idx]], device=device) # Ensure new token is on device\n",
        "\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    return \" \".join(result)\n",
        "\n",
        "\n",
        "# --- Main Execution Flow ---\n",
        "\n",
        "# Build vocabularies for seq2seq\n",
        "eng_vocab_seq2seq = build_vocab_seq2seq(english_sentences)\n",
        "yor_vocab_seq2seq = build_vocab_seq2seq(yoruba_sentences)\n",
        "print(f\"English seq2seq vocabulary size: {len(eng_vocab_seq2seq)}\")\n",
        "print(f\"Yoruba seq2seq vocabulary size: {len(yor_vocab_seq2seq)}\")\n",
        "\n",
        "\n",
        "# Use CBOW to learn embeddings and save them\n",
        "EMBD_DIM = 64\n",
        "train_cbow(open(\"/content/sample_data/english.txt\").readlines(), \"/content/sample_data/eng_embeddings.pt\", embedding_dim=EMBD_DIM, epochs = 1000)\n",
        "train_cbow(open(\"/content/sample_data/yoruba.txt\").readlines(), \"/content/sample_data/yor_embeddings.pt\", embedding_dim=EMBD_DIM, epochs = 1000)\n",
        "\n",
        "# Load embeddings\n",
        "eng_embed_data = torch.load(\"/content/sample_data/eng_embeddings.pt\")\n",
        "yor_embed_data = torch.load(\"/content/sample_data/yor_embeddings.pt\")\n",
        "\n",
        "eng_embeddings = eng_embed_data[\"embeddings\"]\n",
        "yor_embeddings = yor_embed_data[\"embeddings\"]\n",
        "print(\"Embeddings loaded.\")\n",
        "\n",
        "HIDDEN_DIM = 128\n",
        "NUM_LAYERS = 2\n",
        "\n",
        "# Initialize the encoder and the decoder with num_layers=4 and move to device\n",
        "encoder = Encoder(\n",
        "    vocab_size=len(eng_vocab_seq2seq),\n",
        "    emb_dim=EMBD_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    num_layers=NUM_LAYERS # Explicitly set num_layers to 4\n",
        ").to(device)\n",
        "\n",
        "decoder = Decoder(\n",
        "    vocab_size=len(yor_vocab_seq2seq),\n",
        "    emb_dim=EMBD_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    num_layers=NUM_LAYERS # Explicitly set num_layers to 4\n",
        ").to(device)\n",
        "print(f\"Encoder and Decoder initialized with {encoder.num_layers} layers and moved to {device}.\")\n",
        "\n",
        "\n",
        "# Load embeddings into the encoder and decoder\n",
        "encoder.embedding.weight.data[:eng_embeddings.size(0)] = eng_embeddings.to(device)\n",
        "decoder.embedding.weight.data[:yor_embeddings.size(0)] = yor_embeddings.to(device)\n",
        "\n",
        "# Freeze embedding layers\n",
        "encoder.embedding.weight.requires_grad = False\n",
        "decoder.embedding.weight.requires_grad = False\n",
        "print(\"Embeddings loaded into models and frozen.\")\n",
        "\n",
        "\n",
        "# Create the seq2seq Dataset and DataLoader\n",
        "dataset = seq2seqDataset(\n",
        "    english_sentences,\n",
        "    yoruba_sentences,\n",
        "    eng_vocab_seq2seq,\n",
        "    yor_vocab_seq2seq\n",
        ")\n",
        "\n",
        "BATCH_SZE = 16\n",
        "\n",
        "loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_batch\n",
        ")\n",
        "print(\"Seq2seq dataset and dataloader created.\")\n",
        "\n",
        "# Train seq2seq model with increased layers and GPU\n",
        "train_seq2seq(\n",
        "    encoder=encoder,\n",
        "    decoder=decoder,\n",
        "    loader=loader,\n",
        "    epochs= 300 # Retrain for 300 epochs\n",
        ")\n",
        "print(\"Seq2seq model training complete.\")\n",
        "\n",
        "# Evaluate translated output\n",
        "sample_sentence = \"If the devil could organize to fight in heaven\"\n",
        "translation = translate(\n",
        "    sample_sentence,\n",
        "    encoder,\n",
        "    decoder,\n",
        "    eng_vocab_seq2seq,\n",
        "    yor_vocab_seq2seq\n",
        ")\n",
        "\n",
        "print(f\"\\nOriginal English: {sample_sentence}\")\n",
        "print(f\"Translated Yoruba: {translation}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IAo95Ks0lAu"
      },
      "outputs": [],
      "source": [
        "# Evaluate translated output\n",
        "sample_sentence = \"the bible says\"\n",
        "translation = translate(\n",
        "    sample_sentence,\n",
        "    encoder,\n",
        "    decoder,\n",
        "    eng_vocab_seq2seq,\n",
        "    yor_vocab_seq2seq\n",
        ")\n",
        "\n",
        "print(f\"\\nOriginal English: {sample_sentence}\")\n",
        "print(f\"Translated Yoruba: {translation}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
